<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>sisin's blog</title>
        <link>http://sisinhuang.github.io/</link>
        <description><![CDATA[]]></description>
        <atom:link href="http://sisinhuang.github.io/rss.xml" rel="self"
                   type="application/rss+xml" />
        <lastBuildDate>Mon, 2 Sep 2019 00:00:00 UTC</lastBuildDate>


<item>
    <title>理解LSTM网络</title>
    <link>http://sisinhuang.github.io/posts/2019-9-Understanding-LSTM-Networks/index.html</link>
    <description><![CDATA[
<p>Humans don’t start their thinking from scratch every second. As you read this essay, you understand each word based on your understanding of previous words. You don’t throw everything away and start thinking from scratch again. Your thoughts have persistence.</p>
<p>Traditional neural networks can’t do this, and it seems like a major shortcoming. For example, imagine you want to classify what kind of event is happening at every point in a movie. It’s unclear how a traditional neural network could use its reasoning about previous events in the film to inform later ones.</p>
<p>Recurrent neural networks address this issue. They are networks with loops in them, allowing information to persist... <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/index.html">Read more.</a></p>
]]></description>
    <pubDate>Mon, 2 Sep 2019 00:00:00 UTC</pubDate>
    <guid>http://sisinhuang.github.io/posts/2019-9-Understanding-LSTM-Networks/index.html</guid>
</item>

    </channel>
</rss>
